# Sonobuoy Plugins

* [Overview][0]
* [Plugin Definition][1]
  * [Under the Hood][2]
  * [Example][3]
  * [Parameter Reference][4]
* [Available Plugins][5]

## Overview

In addition to querying API objects, Sonobuoy also supports a *Plugin model*. In this model, "worker" pods are dispatched into the cluster to collect data from each node, and use an aggregation URL to submit their results back to a waiting "master" pod. See the diagram below:

![sonobuoy plugins diagram][6]

There are two main components that specify plugin behavior:

1. **Plugin Selection**: A section in the main config (`config.json`) that declares *which* plugins should be used in the Sonobuoy run.

    *These configs are defined by the **end user**.*

1. **Plugin Definition**: Templatized YAML that defines a Kubernetes API resource like a Pod. Plugin metadata--such as a plugin's features, method of launch, and other configurations--are specified with annotations.

    *This YAML is defined by the plugin **developer**, and can be taken as a given by the end user.*

The remainder of this document focuses on **Plugin Definition**.

## Plugin Definition

The *plugin definition* is a YAML file (raw or wrapped in a ConfigMap) that describes the core parts of your custom plugin. This YAML defines a Kubernetes API resource like a Pod.

Some of the values in this YAML file are template variables that will be filled in by Sonobuoy. The template will have access to several variables:

| Variable | Source |  Description |
| --- | --- | --- |
| `{{.SessionID}}` | Autogenerated by Sonobuoy | A generated token used to be able to uniquely identify all pieces of a single sonobuoy run. |
| `{{.Namespace}}` | Populated from the PluginNamespace value in config.json | The Kubernetes namespace the plugins will run in. This value is populated by the `PluginNamespace` value in `config.json`. |
| `{{.MasterAddress}}` | Autogenerated by Sonobuoy | The address the sonobuoy-workers use to communicate with the sonobuoy master process. It is dynamically generated. |

The plugin system relies on some annotations being present in a plugin definition:

| Annotation | Description | Example Values |
| ---  | --- | --- |
| `sonobuoy-driver` | Sonobuoy implements *plugin drivers* that define different modes of operation.<br><br>(1) **"Job" driver**: The plugin runs on a single node (e.g. master).<br>(2) **"DaemonSet" driver**: The plugin runs on each cluster node.<br><br>You can find the implementations [here][7]. | "Job&#124;DaemonSet" |
| `sonobuoy-plugin` | A name that is used to identify the plugin (e.g. in the [Plugin Selection][18]). | "e2e", "systemd_logs" |
| `sonobuoy-result-type` | The name of the subdirectory that this plugin's results are saved in. With a `resultType` of "e2e", results are written into `plugins/e2e/...` (within the tarball output).<br><br>This value is typically the same as the `sonobuoy-plugin` annotation. | "e2e" |

Sonobuoy searches for these definitions in three locations by default:

- `/etc/sonobuoy/plugins.d`
- `$HOME/.sonobuoy/plugins.d`
- `./plugins.d`

This search path can be overridden by the `PluginSearchPath` value of the main Sonobuoy config.

In the [quickstart example][12], the necessary YAML configs are wrapped in a ConfigMap, so that they can be mounted as files in the `/etc/sonobuoy/plugins.d` directory. *This is the recommended approach.*

### Under the Hood

While you can create a plugin without leveraging a Sonobuoy worker container, you will need to write your own aggregation code in that scenario. To make implementation easier, **we recommend using a two-container model**, as described below:

1. **A "Producer" Container**: A containerized way to perform the task that you want executed.

    *This is done by your own container, which does not need to be Sonobuoy-aware*.

1. **A "Consumer" Container**: A way of submitting results back to the aggregation URL, so that the master Sonobuoy instance can collect node-specific data.

    *This is done by a container running `sonobuoy worker`, which can handle the uploading for you.*

1. **A registry to store these compiled image(s)**. When they are ready to be used, the plugin's container image(s) should either be (1) uploaded to a public Docker registry or (2) a private registry, provided that the [appropriate image pull secrets][8] are created in advance.

To allow your two containers to interact (i.e. for the consumer to read and publish the results of the producer), you will need to ensure the following:
* The same `results` volume is mounted onto both containers.

* In the `results` directory (mounted above), the "producer" container writes the path of its output subdirectory to the `done` file.

    For instance, the `systemd_logs` plugin includes the following code in its [bash script][13]:

    ```
    echo -n "${RESULTS_DIR}/systemd_logs" >"${RESULTS_DIR}/done"
    ```




### Example

Below is the Pod template from the [`e2e` plugin definition][9]. It follows the two-container-paradigm. One container runs the e2e tests and writes to a temporary folder, and the other container (a Sonobuoy worker) reads those results and sends them back to the master Sonobuoy instance.

See [Parameter Reference][4] for descriptions of each of the important config keys.

```
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    sonobuoy-driver: Job
    sonobuoy-plugin: e2e
    sonobuoy-result-type: e2e
  labels:
    component: sonobuoy
    sonobuoy-run: '{{.SessionID}}'
    tier: analysis
  name: sonobuoy-e2e-job-{{.SessionID}}
  namespace: '{{.Namespace}}'
spec:
  containers:
  - env:
    - name: E2E_FOCUS
      value: Pods should be submitted and removed
    image: gcr.io/heptio-images/kube-conformance:latest
    imagePullPolicy: Always
    name: e2e
    volumeMounts:
    - mountPath: /tmp/results
      name: results
      readOnly: false
  - command:
    - sh
    - -c
    - /sonobuoy worker global -v 5 --logtostderr
    env:
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: RESULTS_DIR
      value: /tmp/results
    - name: MASTER_URL
      value: '{{.MasterAddress}}'
    - name: RESULT_TYPE
      value: e2e
    image: gcr.io/heptio-images/sonobuoy:master
    imagePullPolicy: Always
    name: sonobuoy-worker
    volumeMounts:
    - mountPath: /tmp/results
      name: results
      readOnly: false
  restartPolicy: Never
  serviceAccountName: sonobuoy-serviceaccount
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  - key: CriticalAddonsOnly
    operator: Exists
  volumes:
  - emptyDir: {}
    name: results
```

### Parameter Reference

| Field | Description |
| --- | --- |
| `tolerations` | A way of specifying node affinity (e.g.  if your plugin should be able to run on master). See [official Kubernetes documentation][10] for more details. |
| `restartPolicy` | Specifies whether your plugin should retry on failure.
| `container.image`, `container.imagePullPolicy` | What and how often a new image should be pulled. |
| `container.env` | Set environmental variables here. These variables can be used to configure plugin behavior.<br><br>For DaemonSet plugins (e.g. `systemdlogs`), the `sonobuoy worker` consumer container needs a `NODE_NAME` variable to know which node the results should be uploaded for.|
| `container.volumeMounts`, `volumes` | **Write results locally**, such that the Sonobuoy worker can find the files it needs to upload to the Sonobuoy master. Typically the same `emptyDir` `results` directory is shared by both the "plugin" container and the Sonobuoy worker container.<br><br> |

## Available Plugins

The current, default set of Sonobuoy plugins are available in the `plugins.d` directory within this repo. You can also use the list below as a reference:

| Plugin | Overview | Source Code Repository | Env Variables (Config) |
| --- | --- | --- | --- |
| [`systemd_logs`][11] | Gather the latest system logs from each node, using systemd's `journalctl` command. | [heptio/sonobuoy-plugin-systemd-logs][16] | (1) `RESULTS_DIR`<br>(2)`CHROOT_DIR`<br>(3)`LOG_MINUTES`|
| [`e2e`][9] | Run Kubernetes end-to-end tests (e.g. conformance) and gather the results. | [heptio/kube-conformance][17] | `E2E_*` variables configure the end-to-end tests. See the [conformance testing guide][15] for details. |
| [`bulkhead`][19] | Perform CIS Benchmark scans from each node using Aqua Security's [`kube-bench`][20] tool. | [bgeesaman/sonobuoy-plugin-bulkhead][19] | (1) `RESULTS_DIR`|

See the [`/build`][14] directory for the source code used to build these plugins (specifically, their "producer" containers).

[0]: #overview
[1]: #developer-plugin-definition
[2]: #under-the-hood
[3]: #example
[4]: #parameter-reference
[5]: #available-plugins
[6]: img/sonobuoy-plugins.png
[7]: /pkg/plugin/driver
[8]: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
[9]: /plugins.d/e2e.tmpl
[10]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature
[11]: /plugins.d/systemdlogs.yaml
[12]: /examples/quickstart.yaml
[13]: https://github.com/heptio/sonobuoy-plugin-systemd-logs/blob/master/get_systemd_logs.sh
[14]: /build
[15]: conformance-testing.md#integration-with-sonobuoy
[16]: https://github.com/heptio/sonobuoy-plugin-systemd-logs
[17]: https://github.com/heptio/kube-conformance
[18]: /docs/configuration.md#plugin-configuration
[19]: https://github.com/bgeesaman/sonobuoy-plugin-bulkhead
[20]: https://github.com/aquasecurity/kube-bench
